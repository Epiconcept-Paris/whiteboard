---
- name: spark sources directory
  tags: install_spark
  file:
    state: directory
    path: "{{ item }}"
    owner: hadoop
    group: hadoop
    mode: 0751
  with_items:
  -  "{{ hadoop_install }}/spark-{{ spark_ver }}-src"
  -  "{{ hadoop_install }}/spark-{{ spark_ver }}"
- name: download spark if does not exists
  tags: install_spark
  command: "wget -O spark-{{ spark_ver }}-src.tgz http://mirrors.ircam.fr/pub/apache/spark/spark-{{ spark_ver }}/spark-{{ spark_ver }}.tgz"
  args:
    chdir: "{{ hadoop_install }}"
    creates: "{{ hadoop_install }}/spark-{{ spark_ver }}-src.tgz"
- name: uncompress spark
  tags: install_spark
  unarchive:
    src: "{{ hadoop_install }}/spark-{{ spark_ver }}-src.tgz"
    dest: "{{ hadoop_install }}/spark-{{ spark_ver }}-src"
    remote_src: true 
    owner: hadoop
    group: hadoop
    mode: 0751
- name: Compile spark
  tags: install_spark
  shell: |
    export JAVA_HOME="{{ java_home  }}"
    export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"
    ./dev/make-distribution.sh --name demy_spark -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.0 -Phive -Phive-thriftserver -Psparkr -DskipTests -Dhttp.proxyHost={{ proxy_host }} -Dhttp.proxyPort={{ proxy_port }} -Dhttps.proxyHost={{ proxy_host }} -Dhttps.proxyPort={{ proxy_port }}
  args:
    chdir: "{{ hadoop_install }}/spark-{{ spark_ver }}-src/spark-{{ spark_ver }}/"
    creates: "{{ hadoop_install }}/spark-{{ spark_ver }}-src/spark-{{ spark_ver }}/dist/README.md"
- name: make compiled files accessible for spark
  tags: install_spark
  file:
    path: "{{ hadoop_install }}/spark-{{ spark_ver }}-src/spark-{{ spark_ver }}/dist"
    state: directory
    group: hadoop
    owner: hadoop
    recurse: true
- name: spark versionned home folder present
  tags: install_spark
  file:
    path: "{{ hadoop_install }}/spark-{{ spark_ver }}" 
    owner: hadoop
    group: hadoop
    state: directory
- name: copy compiled version (no overwrite)
  tags: install_spark
  shell: "cp -R -p -v -n {{ hadoop_install }}/spark-{{ spark_ver }}-src/spark-{{ spark_ver }}/dist/*  {{ hadoop_install }}/spark-{{ spark_ver }} | wc -l"
  register: out
  changed_when: out.stdout!='0'
- name: create link to current version
  tags: install_spark
  file:
    src: "{{ hadoop_install }}/spark-{{ spark_ver }}" 
    dest: "{{ spark_home }}"
    owner: hadoop
    group: hadoop
    mode: 0751
    state: link
- name: spark run exists
  tags: install_spark
  file:
    path: "{{ spark_run }}" 
    owner: spark
    group: hadoop
    state: directory
- name: "jars packaged"
  tags: install_spark
  archive:
    path: "{{ spark_home }}/jars/*"
    dest: "{{ spark_home }}/spark-jars.tgz"
- name: "create folders"
  tags: install_spark
  shell: |
    sudo -u hdfs {{ hadoop_home}}/bin/hdfs dfs -test -d /tmp
    if [ $? -ne 0 ]; then 
      ret="CHANGED"
      sudo -u hdfs {{ hadoop_home}}/bin/hdfs dfs -mkdir /tmp 
      sudo -u hdfs {{ hadoop_home}}/bin/hdfs dfs -chown hadoop:hadoop /tmp 
      sudo -u hdfs {{ hadoop_home}}/bin/hdfs dfs -chmod 0777 /tmp 
    else
      ret="NO-CHANGE"
    fi
    echo $ret
    exit 0
  register: out
  changed_when: not out.stdout.endswith('NO-CHANGE')
  failed_when: out.stderr|length>0 
- name: "upload jar if changed"
  tags: install_spark
  shell: |
    newmd5=`find {{ spark_home}}/jars/ -type f -print0 | xargs -0 md5sum | md5sum | xargs`
    sudo -u hdfs {{ hadoop_home}}/bin/hdfs dfs -test -d /spark
    if [ $? -ne 0 ]; then 
      ret="CHANGED"
      sudo -u hdfs {{ hadoop_home}}/bin/hdfs dfs -mkdir /spark 
      sudo -u hdfs {{ hadoop_home}}/bin/hdfs dfs -chown spark:hadoop /spark 
    fi;
    sudo -u hdfs {{ hadoop_home}}/bin/hdfs dfs -test -e /spark/spark-jars.tgz.md5
    if [ $? -ne 0 ]; then 
      ret="CHANGED"
      oldmd5="not a md5"
    else
      oldmd5="`sudo -u spark {{ hadoop_home}}/bin/hdfs dfs -cat /spark/spark-jars.tgz.md5`"
    fi;
    if [ "$newmd5" != "$oldmd5" ];then
      ret="CHANGED"
      echo $newmd5 | sudo -u spark {{ hadoop_home }}/bin/hdfs dfs -put -f - /spark/spark-jars.tgz.md5
      sudo -u spark {{ hadoop_home }}/bin/hdfs dfs -put -f {{ spark_home }}/spark-jars.tgz /spark/spark-jars.tgz
    else
      ret="NO-CHANGE"
    fi
    echo $ret
    exit 0
  register: out
  changed_when: not out.stdout.endswith('NO-CHANGE')
  failed_when: out.stderr|length>0 
- name: spark environment file present
  tags: configure_spark
  copy:
    content: "#!/usr/bin/env bash"
    dest: "{{ spark_home }}/conf/spark-env.sh"
    force: false
    owner: "spark"
    group: "hadoop"
    mode: "0770"
- name: get ip to use
  tags: configure_spark
  shell: "ip addr show {{ app_network  }} | grep \"inet \" | awk {'print $2'} | awk -F \"/\" {'print $1'}" 
  register: app_ip
  changed_when: false
- tags: configure_spark
  set_fact:
    app_ip: "{{ app_ip.stdout }}"
- name: update spark-env.sh
  tags: configure_spark
  lineinfile:
    path: "{{ spark_home }}/conf/spark-env.sh" 
    regexp: "^export {{ item.var  }}"
    line: "export {{ item.var }}={{ item.value }}"
  with_items:
  - {var: "JAVA_HOME", value: "{{ java_home_spark  }}" }
  - {var: "HADOOP_CONF_DIR", value: "{{ hadoop_conf_dir  }}" }
  - {var: "SPARK_LOCAL_DIRS", value: "{{ spark_run }}" }
  - {var: "SPARK_LOCAL_IP", value: "{{ app_ip }}" }
- name: spark defaults present 
  tags: configure_spark
  copy:
    content: ""
    dest: "{{ spark_home }}/conf/spark-defaults.conf"
    force: false
    owner: "spark"
    group: "hadoop"
    mode: "0660"
- name: update spark-defaults.conf
  tags: configure_spark
  lineinfile:
    path: "{{ spark_home }}/conf/spark-defaults.conf" 
    regexp: "^{{ item.var  }}"
    line: "{{ item.var }} {{ item.value }}"
  with_items:
  - {var: "spark.eventLog.dir", value: "hdfs:///spark/logs" }
  - {var: "spark.yarn.archive", value: "hdfs:///spark/spark-jars.tgz" }
  - {var: "spark.yarn.stagingDir", value: "/spark" }
- name: getting zeppelin
  tags: install_zeppelin
  become_user: hadoop
  git:
    repo: https://github.com/apache/zeppelin.git
    dest: "{{ hadoop_install }}/zepellin-{{ zeppelin_version }}-src"
    version: "{{ zeppelin_version }}"
- name: copy sources (no overwrite)
  tags: install_zeppelin
  become_user: hadoop
  shell: "cp -R -p -v -n {{ hadoop_install }}/zepellin-{{ zeppelin_version }}-src {{ hadoop_install }}/zepellin-{{ zeppelin_version }} | wc -l"
  register: out
  changed_when: out.stdout!='0'
- name: ensure bower has a proper proxy 
  tags: install_zeppelin
  copy:
    content: |
      {
      "proxy":"http://{{ proxy_host }}:{{ proxy_port }}",
      "https-proxy":"http://{{ proxy_host }}:{{ proxy_port }}"
      }
    dest: /home/hadoop/.bowerrc    
- name: install R dependencies for zeppelin
  tags: install_zeppelin
  shell: |
    export http_proxy=http://{{ proxy_host }}:{{ proxy_port }}
    export https_proxy=http://{{ proxy_host }}:{{ proxy_port }}
    export ftp_proxy={{ proxy_host }}:{{ proxy_port }}
    R -e "list.of.packages <- c(\"evaluate\"); new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]; if(length(new.packages)) install.packages(new.packages)"
  register: out
  failed_when: out.stderr.find("WARNING")>1
  changed_when: out.stderr|length > 0
- name: build zeppelin 
  tags: install_zeppelin
  become_user: hadoop
  shell: | 
    export HTTP_PROXY=http://{{ proxy_host }}:{{ proxy_port }}
    export HTTPS_PROXY=http://{{ proxy_host }}:{{ proxy_port }}
    dev/change_scala_version.sh 2.11
    mvn clean package -DskipTests -Pspark-{{ spark_ver[:3]}} -Phadoop-2.7 -Pyarn -Ppyspark -Psparkr -Pr -Pscala-2.11 -Dhttp.proxyHost={{ proxy_host }} -Dhttp.proxyPort={{ proxy_port }} -Dhttps.proxyHost={{ proxy_host }} -Dhttps.proxyPort={{ proxy_port }}
  args:
    chdir: "{{ hadoop_install }}/zepellin-{{ zeppelin_version }}"
    creates: "{{ hadoop_install }}/zepellin-{{ zeppelin_version }}/target"

