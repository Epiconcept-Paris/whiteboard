---
- name: crypted disk is running
  tags: check_hosts
  shell: "if mountpoint -q /{{ crypted_disk }}; then echo 'MOUNTED'; else echo 'NOT-MOUNTED'; fi"
  register: mount_output
  changed_when: false
- name: start crypted disk
  tags: check_hosts
  shell: |
    echo -n "{{ decrypt_disk_pass }}" | cryptsetup luksOpen --key-file=- /dev/{{ hostname.stdout }}-crypt-1/{{ crypted_disk  }} {{ crypted_disk  }}
  args:
    executable: /bin/bash
  register: out
  changed_when: out.stdout=='' and out.stderr=''
  failed_when: out.stderr != '' and not out.stderr.endswith('already exists.')
  when: mount_output.stdout == 'NOT-MOUNTED'
- name: mount crypted disk 
  tags: check_hosts
  shell: "mount /{{ crypted_disk  }}"
  register: out
  changed_when: true 
  when: mount_output.stdout == 'NOT-MOUNTED'
- name: IPv6 disables
  tags: check_hosts
  lineinfile:
    path: "/etc/sysctl.conf" 
    regexp: '^net.ipv6.conf.all.disable_ipv6'
    line: "net.ipv6.conf.all.disable_ipv6 = 1"
  notify: apply_sysctl    
- name: get ip to use
  shell: "ip addr list | grep \"{{ front_network  }}\\|{{ back_network  }}\" | grep \"inet \" | awk {'print $2'} | awk -F \"/\" {'print $1'}" 
  register: app_ip
  changed_when: false
  tags: etc_hosts
- set_fact:
    app_ip: "{{ app_ip.stdout }}"
  tags: etc_hosts
- name: set ips alias on /etc/hosts
  blockinfile:
    path: /etc/hosts
    block: |
      {{hostvars[item].app_ip}} {{ item.split('.')[0] }}-{{ hadoop_cluster_name }}
    marker: "# {mark} Managed ip for {{ item }} alias do not edit!" 
  with_items : "{{ groups['nodes'] }}"
  tags: etc_hosts
- name: installation folder
  file:
    path: "{{ hadoop_install }}"
    state: directory
    mode: 0751
    owner: hadoop
    group: hadoop
- name: jessie backports is activated
  apt_repository:
    repo: deb http://ftp.fr.debian.org/debian jessie-backports main
- name: intall jessie backports packages
  tags: packages
  apt:
    name: "{{ item }}"
    default_release: jessie-backports
    update_cache: true
    state: latest
  with_items: ["{{ java_package }}", "{{ java_package_spark }}", "maven", "r-base"  ]
  notify: set_java_default
- name: set java home
  tags: java
  lineinfile:
    path: "/etc/environment" 
    regexp: '^JAVA_HOME='
    line: "JAVA_HOME={{ java_home }}"
  notify: set_java_default
- name: compiler tools
  tags: packages
  apt:
    name: "{{ item }}"
    update_cache: yes
    cache_valid_time: 300
  with_items: [build-essential, g++, autoconf, automake, libtool, cmake, zlib1g-dev, pkg-config, libssl-dev, xmlstarlet, git, npm, libfontconfig,pandoc, libgdal-dev, libgeos-dev, libproj-dev, proj-data, locales, curl ]
  notify: [ set_npm_proxy, set_cran_mirror ]
- name: install R packages for Spark
  tags: packages
  shell: |
    export http_proxy=http://{{ proxy_host }}:{{ proxy_port }}
    export https_proxy=http://{{ proxy_host }}:{{ proxy_port }}
    export ftp_proxy={{ proxy_host }}:{{ proxy_port }}
    R -e "list.of.packages <- c(\"{{ spark_r_packages | join('\", \"') }}\"); new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]; if(length(new.packages)) install.packages(new.packages, repos=c('https://cran.univ-paris1.fr','https://ftp.igh.cnrs.fr/pub/CRAN/','http://cran.irsn.fr/'))"
  register: out
  failed_when: out.stderr.find("WARNING")>1
  changed_when: out.stderr|length > 0
  notify: restart zeppelin
- tags: packages
  debug:
    msg: "{{ out }}"
- name: download google protocol buffer
  command: "wget -nc https://github.com/google/protobuf/releases/download/v{{ protobuf_ver }}/protobuf-{{ protobuf_ver }}.tar.gz"
  args:
    chdir: "{{ hadoop_install }}"
    creates: "{{ hadoop_install }}/protobuf-{{ protobuf_ver }}.tar.gz"
  tags: protobuf
- name: uncompress protocol buffers 
  unarchive:
    src: "{{ hadoop_install }}/protobuf-{{ protobuf_ver }}.tar.gz"
    dest: "/usr/local/src"
    remote_src: true 
    creates: "/usr/local/src/protobuf-{{ protobuf_ver }}" 
  tags: protobuf
- name: make/install protocol buffers
  command: "{{ item }}"
  args:
    chdir: "/usr/local/src/protobuf-{{ protobuf_ver }}"
    creates: /usr/bin/protoc
  with_items:
  - ./autogen.sh
  - ./configure --prefix=/usr
  - make
  - make install
  tags: protobuf
- name: install protocols buffers for java
  command: "mvn install -Dhttp.proxyHost={{ proxy_host }} -Dhttp.proxyPort={{ proxy_port }} -Dhttps.proxyHost={{ proxy_host }} -Dhttps.proxyPort={{ proxy_port }}"
  args:
    chdir: "/usr/local/src/protobuf-{{ protobuf_ver }}/java"
    creates: "/usr/local/src/protobuf-{{ protobuf_ver }}/java/target/protobuf-java-{{ protobuf_ver }}.jar"
  tags: protobuf
- meta: flush_handlers
- name: download hadoop if does not exists
  tags: install_hadoop
  command: "wget {{ apache_mirror }}/pub/apache/hadoop/common/hadoop-{{ hadoop_ver }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
  args:
    chdir: "{{ hadoop_install }}"
    creates: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
- name: uncompress hadoop
  tags: install_hadoop
  unarchive:
    src: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
    dest: "{{ hadoop_install }}"
    remote_src: true 
- name: Compile hadoop
  tags: install_hadoop
  command: 'mvn package -Pdist,native -DskipTests -Dtar -Dhttp.proxyHost={{ proxy_host }} -Dhttp.proxyPort={{ proxy_port }} -Dhttps.proxyHost={{ proxy_host }} -Dhttps.proxyPort={{ proxy_port }}'
  args:
    chdir: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src"
    creates: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src/hadoop-dist/target/hadoop-dist-{{ hadoop_ver }}.jar"
- name: copy compiled version (no overwrite)
  tags: install_hadoop
  shell: "cp -R -v -n {{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src/hadoop-dist/target/hadoop-{{ hadoop_ver }} {{ hadoop_install }}/ | wc -l"
  register: out
  changed_when: out.stdout!='0'
- name: create link to current version
  tags: install_hadoop
  file:
    src: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}" 
    dest: "{{ hadoop_home }}"
    owner: hadoop
    group: hadoop
    mode: 0751
    state: link
    #force: true
- name: set hadoop home
  tags: configure_hadoop
  lineinfile:
    path: "/etc/environment" 
    regexp: '^HADOOP_PREFIX='
    line: "HADOOP_PREFIX={{ hadoop_home  }}"
- name: ensure conf directory exists
  tags: configure_hadoop
  file:
    path: "{{ item }}"
    state: directory
    mode: 0750
    owner: hadoop
    group: hadoop
  with_items:
  - "{{ hadoop_conf_dir }}"
- name: ensure log et pid folder exists
  tags: configure_hadoop
  file:
    path: "{{ item }}"
    state: directory
    mode: 0770
    owner: hadoop
    group: hadoop
  with_items:
  - "{{ hadoop_log_dir }}"
  - "{{ hadoop_pid_dir }}"
- name: copy conf files (no overwrite)
  tags: configure_hadoop
  shell: "cp -R -v -n {{ hadoop_home}}/etc/hadoop/* {{ hadoop_conf_dir }} | wc -l"
  register: out
  changed_when: out.stdout!='0'
- name: update hadoop-env.sh
  tags: configure_hadoop
  lineinfile:
    path: "{{ hadoop_conf_dir }}/hadoop-env.sh" 
    regexp: "^export {{ item.var  }}"
    line: "export {{ item.var }}=\"{{ item.value }}\""
  with_items:
  - {var: "JAVA_HOME", value: "{{ java_home  }}" }
  - {var: "HADOOP_LOG_DIR", value: "{{ hadoop_log_dir }}" }
  - {var: "HADOOP_CONF_DIR", value: "{{ hadoop_conf_dir  }}" }
  - {var: "HADOOP_PID_DIR", value: "{{ hadoop_pid_dir  }}" }
  - {var: "HADOOP_NAMENODE_OPTS", value: "-Xmx{{ hdfs_namenode_daemon_memory_mb }}m" }
  - {var: "HADOOP_DATANODE_OPTS", value: "-Xmx{{ hdfs_datanode_daemon_memory_mb }}m" }
  - {var: "YARN_RESOURCEMANAGER_OPTS", value: "-Xmx{{ yarn_resourcemanager_daemon_memory_mb }}m" }
  - {var: "YARN_NODEMANAGER_OPTS", value: "-Xmx{{ yarn_nodemanager_daemon_memory_mb }}m" }
  - {var: "YARN_PROXYSERVER_OPTS", value: "-Xmx{{ yarn_webproxy_daemon_memory_mb  }}m" }
  - {var: "HADOOP_JOB_HISTORYSERVER_OPTS", value: "-Xmx{{ mapreduce_jobhistory_memory_mb }}m" }
  notify: [restart hdfs, restart yarn]
- name: copy script to update hadoop xml files
  tags: configure_hadoop
  copy:
    src: "./xmlpresent.sh"
    dest: "/tmp/xmlpresent.sh"
    owner: hadoop
    group: hadoop
    mode: 0770
- name: update core-site.xml
  tags: configure_hadoop
  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/core-site.xml"
  register: out
  changed_when: not out.stdout.startswith('NO-CHANGE')
  with_items:
  - {var: "fs.defaultFS", value: "hdfs://{{ groups['namenode'][0].split('.')[0] }}-{{ hadoop_cluster_name }}:{{ hdfs_port }}" }
  - {var: "io.file.buffer.size", value: "{{ hdfs_file_buffer_bytes }}" }
  - {var: "hadoop.proxyuser.root.hosts", value: "{{ groups['hdfs_edge'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
  - {var: "hadoop.proxyuser.root.groups", value: "*" }
  - {var: "hadoop.proxyuser.Spark.hosts", value: "{{ groups['hdfs_edge'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
  - {var: "hadoop.proxyuser.spark.groups", value: "*" }
  notify: [restart hdfs, restart yarn]
- name: yarn log folder exist 
  tags: configure_yarn
  file:  
    path: "{{ yarn_log_dir }}"
    state: directory
    mode: 0751
    owner: yarn
    group: hadoop
- name: setting necessary environment variable
  tags: configure_yarn
  lineinfile:
    path: "{{ hadoop_conf_dir }}/yarn-env.sh" 
    regexp: "^export {{ item.var  }}"
    line: "export {{ item.var }}={{ item.value }}"
  with_items:
  - {var: "YARN_LOG_DIR", value: "{{ yarn_log_dir }}"} 
  - {var: "YARN_PID_DIR", value: "{{ hadoop_pid_dir  }}"}
  notify: restart yarn
- name: copy script to update hadoop xml files
  tags: configure_yarn
  copy:
    src: "./xmlpresent.sh"
    dest: "/tmp/xmlpresent.sh"
    owner: hadoop
    group: hadoop
    mode: 0770
- name: update yarn-site.xml for resouerce manager
  tags: configure_yarn
  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/yarn-site.xml"
  register: out
  changed_when: not out.stdout.startswith('NO-CHANGE')
  with_items:
  - {var: "yarn.acl.enable", value: "{{ yarn_acl  }}" }
  - {var: "yarn.admin.acl", value: "{{ yarn_admin_acl  }}" }
  - {var: "yarn.log-aggregation-enable", value: "{{ yarn_log_aggregation }}" }
  - {var: "yarn.resourcemanager.address", value: "{{ groups['resourcemanager'][0].split('.')[0] }}-{{ hadoop_cluster_name }}:{{ yarn_resourcemanager_port }}" }
  - {var: "yarn.resourcemanager.scheduler.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_resourcescheduler_port  }}" }
  - {var: "yarn.resourcemanager.resource-tracker.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_resourcetracker_port }}" }
  - {var: "yarn.resourcemanager.admin.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_admin_port  }}" }
  - {var: "yarn.resourcemanager.webapp.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_webapp_port }}" }
  - {var: "yarn.resourcemanager.scheduler.class", value: "{{ yarn_scheduler }}" }
  - {var: "yarn.scheduler.minimum-allocation-mb", value: "{{ yarn_container_min_mb }}" }
  - {var: "yarn.scheduler.maximum-allocation-mb", value: "{{ yarn_container_max_mb }}" }
  - {var: "yarn.scheduler.maximum-allocation-vcores", value: "{{ yarn_container_max_cores }}" }
  - {var: "yarn.resourcemanager.nodes.include-path", value: "{{ yarn_hosts  }}" }
  - {var: "yarn.resourcemanager.nodes.exclude-path", value: "{{ yarn_hosts_exclude }}" }
  - {var: "yarn.resourcemanager.bind-host", value: "{{ groups['resourcemanager'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
  - {var: "yarn.web-proxy.address", value: "{{ groups['yarn_edge'][0].split('.')[0] }}-{{ hadoop_cluster_name }}:{{ yarn_webproxy_port }}" }
  notify: restart yarn
- name: update capacity-scheduler.xml for resouerce manager
  tags: configure_yarn
  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/capacity-scheduler.xml"
  register: out
  changed_when: not out.stdout.startswith('NO-CHANGE')
  with_items:
  - {var: "yarn.scheduler.capacity.resource-calculator", value: "{{ yarn_scheduler_capacity_calculator }}" }
  notify: restart yarn
- name: update yarn-site.xml on node managers
  tags: configure_yarn
  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/yarn-site.xml"
  register: out
  changed_when: not out.stdout.startswith('NO-CHANGE')
  with_items:
  - {var: "yarn.nodemanager.local-dirs", value: "{{ yarn_nodemanager_local_dirs  }}" }
  - {var: "yarn.nodemanager.log-dirs", value: "{{ yarn_log_dir }}" }
  - {var: "yarn.nodemanager.log.retain-seconds", value: "{{ yarn_nodemanager_log_seconds }}" }
  - {var: "yarn.nodemanager.remote-app-log-dir", value: "{{ yarn_nodemanager_remote_log_dir }}" }
  - {var: "yarn.nodemanager.remote-app-log-dir-suffix", value: "{{ yarn_nodemanager_remote_log_dir_suffix  }}" }
  - {var: "yarn.nodemanager.resource.detect-hardware-capabilities", value: "{{ yarn_nodemanager_detect_hardware }}" }
  - {var: "yarn.nodemanager.resource.memory-mb", value: "{{ yarn_nodemanager_memory_mb  }}" }
  - {var: "yarn.nodemanager.resource.system-reserved-memory-mb", value: "{{ yarn_nodemanager_system_reserved_memory_mb  }}" }
  - {var: "yarn.nodemanager.resource.cpu-vcores", value: "{{ yarn_nodemanager_resource_vcores  }}" }
  - {var: "yarn.nodemanager.resource.count-logical-processors-as-cores", value: "{{ yarn_nodemanager_logical_procs_as_cores }}" }
  - {var: "yarn.nodemanager.resource.pcores-vcores-multiplier", value: "{{ yarn_nodemanager_resource_pcores_multiplier }}" }
  - {var: "yarn.nodemanager.vmem-check-enabled", value: "{{ yarn_nodemanager_check_vmem }}" }
  - {var: "yarn.nodemanager.pmem-check-enabled", value: "{{ yarn_nodemanager_check_pmem }}" }
  - {var: "yarn.nodemanager.vmem-pmem-ratio", value: "{{ yarn_nodemanager_vmem_pmem_ratio  }}" }
  - {var: "yarn.nodemanager.hostname", value: "{{ inventory_hostname.split('.')[0] }}-{{ hadoop_cluster_name }}" }
  - {var: "yarn.nodemanager.bind-host", value: "{{ inventory_hostname.split('.')[0] }}-{{ hadoop_cluster_name }}" }
  notify: restart yarn
- name: get ram
  tags: configure_map_reduce
  shell: "free -m | grep Mem:|awk '{ printf $2}'"
  register: node_ram
  changed_when: false
- name: get cpu
  tags: configure_map_reduce
  command: "nproc --all"
  register: node_cpu
  changed_when: false
- name: copy script to update hadoop xml files
  tags: configure_map_reduce
  copy:
    src: "./xmlpresent.sh"
    dest: "/tmp/xmlpresent.sh"
    owner: hadoop
    group: hadoop
    mode: 0770
- name: copy conf file xml files
  tags: configure_map_reduce
  copy:
    src: "{{ hadoop_conf_dir }}/mapred-site.xml.template"
    dest: "{{ hadoop_conf_dir }}/mapred-site.xml"
    remote_src: True
    force: false
    owner: yarn
    group: hadoop
    mode: 0770
- name: update mapred-site.xml on nodes
  tags: configure_map_reduce
  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/mapred-site.xml"
  register: out
  changed_when: not out.stdout.startswith('NO-CHANGE')
  with_items:
  - {var: "mapreduce.framework.name", value: "{{ mapreduce_framework_name }}" }
  - {var: "mapreduce.map.memory.mb", value: "{{ ((1.5*node_ram.stdout|int) / (node_cpu.stdout|int))|int }}" }
  - {var: "mapreduce.map.java.opts", value: "-Xmx{{ (node_ram.stdout|int / node_cpu.stdout|int)|int }}M" }
  - {var: "mapreduce.reduce.memory.mb", value: "{{ ((2*node_ram.stdout|int) / (node_cpu.stdout|int))|int }}" }
  - {var: "mapreduce.reduce.java.opts", value: "-Xmx{{ (2*node_ram.stdout|int / node_cpu.stdout|int)|int }}M" }
  - {var: "mapreduce.task.io.sort.mb", value: "{{ ((0.5*node_ram.stdout|int) / (node_cpu.stdout|int))|int }}" }
  - {var: "mapreduce.task.io.sort.factor", value: "{{ mapreduce_task_io_sort_factor  }}" }
  - {var: "mapreduce.reduce.shuffle.parallelcopies", value: "{{ mapreduce_reduce_shuffle_parallelcopies  }}" }
  - {var: "mapreduce.jobhistory.address", value: "{{ groups['resourcemanager'][0].split('.')[0]}}:{{mapreduce_jobhistory_port  }}" }
  - {var: "mapreduce.jobhistory.webapp.address", value: "{{ groups['resourcemanager'][0].split('.')[0]}}:{{ mapreduce_jobhistory_webapp_port }}" }
  - {var: "mapreduce.jobhistory.intermediate-done-dir", value: "{{ mapreduce_jobhistory_intermediate_done_dir  }}" }
  - {var: "mapreduce.jobhistory.done-dir", value: "{{ mapreduce_jobhistory_done_dir  }}" }
