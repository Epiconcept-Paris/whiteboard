---
- name: cheking hosts
  hosts: nodes
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: crypted disk is running
    tags: check_hosts
    shell: "if mountpoint -q /{{ crypted_disk }}; then echo 'MOUNTED'; else echo 'NOT-MOUNTED'; fi"
    register: mount_output
    changed_when: false
  - name: start crypted disk
    tags: check_hosts
    shell: |
      echo -n "{{ decrypt_disk_pass }}" | cryptsetup luksOpen --key-file=- /dev/{{ hostname.stdout }}-crypt-1/{{ crypted_disk  }} {{ crypted_disk  }}
    args:
      executable: /bin/bash
    register: out
    changed_when: out.stdout=='' and out.stderr=''
    failed_when: out.stderr != '' and not out.stderr.endswith('already exists.')
    when: mount_output.stdout == 'NOT-MOUNTED'
  - name: mount crypted disk 
    tags: check_hosts
    shell: "mount /{{ crypted_disk  }}"
    register: out
    changed_when: true 
    when: mount_output.stdout == 'NOT-MOUNTED'
  - name: IPv6 disables
    tags: check_hosts
    lineinfile:
      path: "/etc/sysctl.conf" 
      regexp: '^net.ipv6.conf.all.disable_ipv6'
      line: "net.ipv6.conf.all.disable_ipv6 = 1"
    notify: apply_sysctl    
  handlers:
  - name: apply sysctl changes
    listen: apply_sysctl
    command: "sysctl -p /etc/sysctl.conf"

- name: Users on head node & register worker hosts
  hosts: namenode 
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: hadoop group present on head node
    group:
      name: hadoop
    tags : create_users
  - name: hadoop users present on head node
    tags : create_users
    user:
      name: "{{ item }}"
      generate_ssh_key: true
      group: hadoop
    register: created_users
    with_items: "{{ hadoopusers }}"
  - name: global known host present
    tags: create_users
    copy:
      dest: /etc/ssh/ssh_known_hosts
      content: ''
      force: false
  - name: checking hosts already registered
    tags: create_users
    shell: "ssh-keygen -F {{ item.split('.')[0] }} -f /etc/ssh/ssh_known_hosts || true"
    register: host_keys_status
    with_items: "{{ groups['nodes'] }}" 
    changed_when: false
  - name: gathering all missing host keys
    tags: create_users
    command: "ssh-keyscan -t rsa -H {{ item.item.split('.')[0] }}"
    register: host_keys_to_insert
    with_items: "{{ host_keys_status.results }}"
    when: item.stdout == ''
    changed_when: false
    loop_control:
      label: "{{ item.item }}"
  - name: known hosts registered for ssh
    tags: create_users
    lineinfile:
      path: /etc/ssh/ssh_known_hosts
      line: "{{ item.stdout  }} {{ item.item.item.split('.')[0]  }}"
      regexp: "{{ item.item.item.split('.')[0] }}$"
      create: true
    with_items: "{{ host_keys_to_insert.results }}"
    when: item.skipped is undefined
    loop_control:
      label: "{{ item.item.item }}"
- name: worker nodes users & passwordless ssh
  hosts: workernodes
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: users present 
    tags : create_users 
    user: 
      name: "{{ item.name }}"
      group: hadoop
      groups: ssh
    with_items: "{{ hostvars[groups['namenode'][0]].created_users.results }}"
    loop_control:
      label: "{{ item.name }}"
  - name: authorized keys present
    tags: create_users
    authorized_key:
      user: "{{ item.name  }}"
      state: present
      key: "{{ item.ssh_public_key  }}"     
    with_items: "{{ hostvars[groups['namenode'][0]].created_users.results }}"
    loop_control:
      label: "{{ item.name }}"
- name: put hosts ips on /etc/hosts
  hosts: nodes
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: get ip to use
    shell: "ip addr show {{ app_network  }} | grep \"inet \" | awk {'print $2'} | awk -F \"/\" {'print $1'}" 
    register: app_ip
    changed_when: false
    tags: etc_hosts
  - set_fact:
      app_ip: "{{ app_ip.stdout }}"
    tags: etc_hosts
  - name: set ips alias on /etc/hosts
    blockinfile:
      path: /etc/hosts
      block: |
        {{hostvars[item].app_ip}} {{ item.split('.')[0] }}-{{ hadoop_cluster_name }}
      marker: "# {mark} Managed ip for {{ item }} alias do not edit!" 
    with_items : "{{ groups['nodes'] }}"
    tags: etc_hosts
- name: install prerequisites
  hosts: nodes
  gather_facts: false
  roles: 
  - hadoop_commons 
  become: true
  tasks:
  - name: installation folder
    file:
      path: "{{ hadoop_install }}"
      state: directory
      mode: 0751
      owner: hadoop
      group: hadoop
  - name: jessie backports is activated
    apt_repository:
      repo: deb http://ftp.fr.debian.org/debian jessie-backports main
  - name: ensure java open jdk is installed 
    tags: java
    apt:
      name: "{{ java_package }}"
      default_release: jessie-backports
      update_cache: true
    with_items: ["{{ java_package }}", "{{ java_package_spark }}"]
    notify: set_java_default
  - name: set java home
    tags: java
    lineinfile:
      path: "/etc/environment" 
      regexp: '^JAVA_HOME='
      line: "JAVA_HOME={{ java_home }}"
    notify: set_java_default
  - name: compiler tools
    tags: packages
    apt:
      name: "{{ item }}"
      update_cache: yes
      cache_valid_time: 300
    with_items: [maven, build-essential, g++, autoconf, automake, libtool, cmake, zlib1g-dev, pkg-config, libssl-dev, xmlstarlet, r-base, git, npm, libfontconfig ]
    notify: [ set_npm_proxy, set_cran_mirror ]
  - name: download google protocol buffer
    command: "wget -nc https://github.com/google/protobuf/releases/download/v{{ protobuf_ver }}/protobuf-{{ protobuf_ver }}.tar.gz"
    args:
      chdir: "{{ hadoop_install }}"
      creates: "{{ hadoop_install }}/protobuf-{{ protobuf_ver }}.tar.gz"
    tags: protobuf
  - name: uncompress protocol buffers 
    unarchive:
      src: "{{ hadoop_install }}/protobuf-{{ protobuf_ver }}.tar.gz"
      dest: "/usr/local/src"
      remote_src: true 
      creates: "/usr/local/src/protobuf-{{ protobuf_ver }}" 
    tags: protobuf
  - name: make/install protocol buffers
    command: "{{ item }}"
    args:
      chdir: "/usr/local/src/protobuf-{{ protobuf_ver }}"
      creates: /usr/bin/protoc
    with_items:
    - ./autogen.sh
    - ./configure --prefix=/usr
    - make
    - make install
    tags: protobuf
  - name: install protocols buffers for java
    command: "mvn install -Dhttp.proxyHost={{ proxy_host }} -Dhttp.proxyPort={{ proxy_port }} -Dhttps.proxyHost={{ proxy_host }} -Dhttps.proxyPort={{ proxy_port }}"
    args:
      chdir: "/usr/local/src/protobuf-{{ protobuf_ver }}/java"
      creates: "/usr/local/src/protobuf-{{ protobuf_ver }}/java/target/protobuf-java-{{ protobuf_ver }}.jar"
    tags: protobuf
  handlers:
  - name: set java default
    listen: set_java_default
    command: "update-java-alternatives --set /usr/lib/jvm/{{ java_default }}"
  - name: set npm proxy
    listen: set_npm_proxy
    shell: | 
      npm config set proxy http://{{ proxy_host }}:{{ proxy_port }}
      npm config set https-proxy http://{{ proxy_host }}:{{ proxy_port }}
  - name: set CRAN_MIRROR for R
    listen: set_cran_mirror
    blockinfile:
      path: /etc/R/Rprofile.site
      marker: "# {mark} CRAN config (managed by Ansible)"
      block: |
        local({
        # add MASS to the default packages, set a CRAN mirror
        old <- getOption("defaultPackages"); r <- getOption("repos")
        r["CRAN"] <- "{{ cran_mirror  }}"
        options(defaultPackages = c(old, "MASS"), repos = r)
        })
  environment:
    http_proxy: "{{ proxy_host }}:{{ proxy_port }}"
    https_proxy: "{{ proxy_host }}:{{ proxy_port }}"
- name: install hadoop (all nodes)
  hosts: nodes
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: download hadoop if does not exists
    tags: install_hadoop
    command: "wget {{ apache_mirror }}/pub/apache/hadoop/common/hadoop-{{ hadoop_ver }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
    args:
      chdir: "{{ hadoop_install }}"
      creates: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
  - name: uncompress hadoop
    tags: install_hadoop
    unarchive:
      src: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
      dest: "{{ hadoop_install }}"
      remote_src: true 
  - name: Compile hadoop
    tags: install_hadoop
    command: 'mvn package -Pdist,native -DskipTests -Dtar -Dhttp.proxyHost={{ proxy_host }} -Dhttp.proxyPort={{ proxy_port }} -Dhttps.proxyHost={{ proxy_host }} -Dhttps.proxyPort={{ proxy_port }}'
    args:
      chdir: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src"
      creates: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src/hadoop-dist/target/hadoop-dist-{{ hadoop_ver }}.jar"
  - name: copy compiled version (no overwrite)
    tags: install_hadoop
    shell: "cp -R -v -n {{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src/hadoop-dist/target/hadoop-{{ hadoop_ver }} {{ hadoop_install }}/ | wc -l"
    register: out
    changed_when: out.stdout!='0'
  - name: create link to current version
    tags: install_hadoop
    file:
      src: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}" 
      dest: "{{ hadoop_home }}"
      owner: hadoop
      group: hadoop
      mode: 0751
      state: link
      #force: true
  - name: set hadoop home
    tags: configure_hadoop
    lineinfile:
      path: "/etc/environment" 
      regexp: '^HADOOP_PREFIX='
      line: "HADOOP_PREFIX={{ hadoop_home  }}"
  - name: ensure conf directory exists
    tags: configure_hadoop
    file:
      path: "{{ item }}"
      state: directory
      mode: 0750
      owner: hadoop
      group: hadoop
    with_items:
    - "{{ hadoop_conf_dir }}"
  - name: ensure log et pid folder exists
    tags: configure_hadoop
    file:
      path: "{{ item }}"
      state: directory
      mode: 0770
      owner: hadoop
      group: hadoop
    with_items:
    - "{{ hadoop_log_dir }}"
    - "{{ hadoop_pid_dir }}"
  - name: copy conf files (no overwrite)
    tags: configure_hadoop
    shell: "cp -R -v -n {{ hadoop_home}}/etc/hadoop/* {{ hadoop_conf_dir }} | wc -l"
    register: out
    changed_when: out.stdout!='0'
  - name: update hadoop-env.sh
    tags: configure_hadoop
    lineinfile:
      path: "{{ hadoop_conf_dir }}/hadoop-env.sh" 
      regexp: "^export {{ item.var  }}"
      line: "export {{ item.var }}={{ item.value }}"
    with_items:
    - {var: "JAVA_HOME", value: "{{ java_home  }}" }
    - {var: "HADOOP_LOG_DIR", value: "{{ hadoop_log_dir }}" }
    - {var: "HADOOP_CONF_DIR", value: "{{ hadoop_conf_dir  }}" }
    - {var: "HADOOP_PID_DIR", value: "{{ hadoop_pid_dir  }}" }
  - name: copy script to update hadoop xml files
    tags: configure_hadoop
    copy:
      src: "./xmlpresent.sh"
      dest: "/tmp/xmlpresent.sh"
      owner: hadoop
      group: hadoop
      mode: 0770
  - name: update core-site.xml
    tags: configure_hadoop
    command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/core-site.xml"
    register: out
    changed_when: not out.stdout.startswith('NO-CHANGE')
    with_items:
    - {var: "fs.defaultFS", value: "hdfs://{{ groups['namenode'][0].split('.')[0] }}-{{ hadoop_cluster_name }}:{{ hdfs_port }}" }
    - {var: "io.file.buffer.size", value: "{{ hdfs_file_buffer_bytes }}" }
    notify: restart hdfs
  environment:
    http_proxy: "{{ proxy_host }}:{{ proxy_port }}"
    https_proxy: "{{ proxy_host }}:{{ proxy_port }}"
- name: install hdfs (head node)
  hosts: namenode
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: produce expected hosts
    tags: configure_hdfs
    set_fact:
      datanodes_list: "{% for i in groups['datanodes'] %}{{ i.split('.')[0]+'-'+hadoop_cluster_name+'\n' }}{%endfor%}" 
  - name: produce excluded hosts
    tags: configure_hdfs
    set_fact:
      datanodes_excluded_list: "{% for i in groups['datanodes_excluded'] %}{{ i.split('.')[0]+'-'+hadoop_cluster_name+'\n' }}{%endfor%}" 
  - name: allowed/excluded hosts present
    tags: configure_hdfs
    copy:
      content: ""
      dest: "{{ item }}"
      force: false
      owner: "hadoop"
      group: "hadoop"
      mode: "0660"
    with_items:
    - "{{ hdfs_hosts }}"
    - "{{ hdfs_hosts_exclude }}"
  - name: allowed hosts file
    tags: configure_hdfs
    blockinfile:
      path: "{{ hdfs_hosts }}"
      block: "{{ datanodes_list }}"
      marker: "#{mark} list of hosts managed by ansible"
  - name: excluded hosts file
    tags: configure_hdfs
    blockinfile:
      path: "{{ hdfs_hosts_exclude }}"
      block: "{{ datanodes_excluded_list }}"
      marker: "#{mark} list of excluded hosts managed by ansible"
  - name: copy script to update hadoop xml files
    tags: configure_hdfs
    copy:
      src: "./xmlpresent.sh"
      dest: "/tmp/xmlpresent.sh"
      owner: hadoop
      group: hadoop
      mode: 0770
  - name: get ip to use
    tags: configure_hdfs
    shell: "ip addr show {{ app_network  }} | grep \"inet \" | awk {'print $2'} | awk -F \"/\" {'print $1'}" 
    register: app_ip
    changed_when: false
  - tags: configure_hdfs
    set_fact:
      app_ip: "{{ app_ip.stdout }}"
  - name: update hdfs-site.xml on head node
    tags: configure_hdfs
    command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/hdfs-site.xml"
    register: out
    changed_when: not out.stdout.startswith('NO-CHANGE')
    with_items:
    - {var: "dfs.client.local.interfaces", value: "{{ app_network }}" }
    - {var: "dfs.namenode.name.dir", value: "file://{{ hdfs_namenode_dir }}" }
    - {var: "dfs.hosts", value: "{{ hdfs_hosts }}" }
    - {var: "dfs.hosts.exclude", value: "{{ hdfs_hosts_exclude }}" }
    - {var: "dfs.blocksize", value: "{{ hdfs_blocksize_bytes }}" }
    - {var: "dfs.namenode.handler.count", value: "{{ hdfs_namenode_thread_count }}" }
    - {var: "dfs.replication", value: "{{ hdfs_default_replication }}" }
    - {var: "dfs.namenode.rpc-bind-host", value: "{{ groups['namenode'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
    - {var: "dfs.namenode.servicerpc-bind-host", value: "{{ groups['namenode'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
    notify: restart hdfs
- name: install hdfs (data node)
  hosts: datanodes
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: update hdfs-site.xml on data node
    tags: configure_hdfs
    command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/hdfs-site.xml"
    register: out
    changed_when: not out.stdout.startswith('NO-CHANGE')
    with_items:
    - {var: "dfs.datanode.data.dir", value: "{{ hdfs_datanode_dir }}" }
    - {var: "dfs.client.local.interfaces", value: "{{ app_network }}" }
    - {var: "dfs.datanode.hostname", value: "{{ inventory_hostname.split('.')[0] }}-{{ hadoop_cluster_name }}"}
    notify: restart hdfs
- name: install yarn (resource manager nodes)
  hosts: resourcemanager
  gather_facts: false
  become: true
  tasks:
  - name: produce expected hosts
    tags: configure_yarn
    set_fact:
      nodemanagers_list: "{% for i in groups['nodemanagers'] %}{{ i.split('.')[0]+'-'+hadoop_cluster_name+'\n' }}{%endfor%}" 
  - name: produce excluded hosts
    tags: configure_yarn
    set_fact:
      nodemanagers_excluded_list: "{% for i in groups['nodemanagers_excluded'] %}{{ i.split('.')[0]+'-'+hadoop_cluster_name+'\n' }}{%endfor%}" 
  - name: allowed/excluded hosts present
    tags: configure_yarn
    copy:
      content: ""
      dest: "{{ item }}"
      force: false
      owner: "yarn"
      group: "hadoop"
      mode: "0660"
    with_items:
    - "{{ yarn_hosts }}"
    - "{{ yarn_hosts_exclude }}"
  - name: allowed hosts file
    tags: configure_yarn
    blockinfile:
      path: "{{ yarn_hosts }}"
      block: "{{ nodemanagers_list }}"
      marker: "#{mark} list of hosts managed by ansible"
  - name: excluded hosts file
    tags: configure_yarn
    blockinfile:
      path: "{{ yarn_hosts_exclude }}"
      block: "{{ nodemanagers_excluded_list }}"
      marker: "#{mark} list of excluded hosts managed by ansible"
- name: configure yarn (global)
  hosts: nodes
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: yarn log folder exist 
    tags: configure_yarn
    file:  
      path: "{{ yarn_log_dir }}"
      state: directory
      mode: 0751
      owner: yarn
      group: hadoop
  - name: setting necessary environment variable
    tags: configure_yarn
    lineinfile:
      path: "{{ hadoop_conf_dir }}/yarn-env.sh" 
      regexp: "^export {{ item.var  }}"
      line: "export {{ item.var }}={{ item.value }}"
    with_items:
    - {var: "YARN_LOG_DIR", value: "{{ yarn_log_dir }}"} 
    - {var: "YARN_PID_DIR", value: "{{ hadoop_pid_dir  }}"}
    notify: restart yarn
- name: configure yarn (resource manager)
  hosts: nodes
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: copy script to update hadoop xml files
    tags: configure_yarn
    copy:
      src: "./xmlpresent.sh"
      dest: "/tmp/xmlpresent.sh"
      owner: hadoop
      group: hadoop
      mode: 0770
  - name: update yarn-site.xml for resouerce manager
    tags: configure_yarn
    command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/yarn-site.xml"
    register: out
    changed_when: not out.stdout.startswith('NO-CHANGE')
    with_items:
    - {var: "yarn.acl.enable", value: "{{ yarn_acl  }}" }
    - {var: "yarn.admin.acl", value: "{{ yarn_admin_acl  }}" }
    - {var: "yarn.log-aggregation-enable", value: "{{ yarn_log_aggregation }}" }
    - {var: "yarn.resourcemanager.address", value: "{{ groups['resourcemanager'][0].split('.')[0] }}-{{ hadoop_cluster_name }}:{{ yarn_resourcemanager_port }}" }
    - {var: "yarn.resourcemanager.scheduler.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_resourcescheduler_port  }}" }
    - {var: "yarn.resourcemanager.resource-tracker.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_resourcetracker_port }}" }
    - {var: "yarn.resourcemanager.admin.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_admin_port  }}" }
    - {var: "yarn.resourcemanager.webapp.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_webapp_port }}" }
    - {var: "yarn.resourcemanager.scheduler.class", value: "{{ yarn_scheduler }}" }
    - {var: "yarn.scheduler.minimum-allocation-mb", value: "{{ yarn_container_min_mb }}" }
    - {var: "yarn.scheduler.maximum-allocation-mb", value: "{{ yarn_container_max_mb }}" }
    - {var: "yarn.scheduler.maximum-allocation-vcores", value: "{{ yarn_container_max_cores }}" }
    - {var: "yarn.resourcemanager.nodes.include-path", value: "{{ yarn_hosts  }}" }
    - {var: "yarn.resourcemanager.nodes.exclude-path", value: "{{ yarn_hosts_exclude }}" }
    - {var: "yarn.resourcemanager.bind-host", value: "{{ groups['resourcemanager'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
    notify: restart yarn
- name: configure yarn (node manager)
  hosts: nodemanagers
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: update yarn-site.xml on node managers
    tags: configure_yarn
    command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/yarn-site.xml"
    register: out
    changed_when: not out.stdout.startswith('NO-CHANGE')
    with_items:
    - {var: "yarn.nodemanager.local-dirs", value: "{{ yarn_nodemanager_local_dirs  }}" }
    - {var: "yarn.nodemanager.log-dirs", value: "{{ yarn_log_dir }}" }
    - {var: "yarn.nodemanager.log.retain-seconds", value: "{{ yarn_nodemanager_log_seconds }}" }
    - {var: "yarn.nodemanager.remote-app-log-dir", value: "{{ yarn_nodemanager_remote_log_dir }}" }
    - {var: "yarn.nodemanager.remote-app-log-dir-suffix", value: "{{ yarn_nodemanager_remote_log_dir_suffix  }}" }
    - {var: "yarn.nodemanager.resource.detect-hardware-capabilities", value: "{{ yarn_nodemanager_detect_hardware }}" }
    - {var: "yarn.nodemanager.resource.memory-mb", value: "{{ yarn_nodemanager_memory_mb  }}" }
    - {var: "yarn.nodemanager.resource.system-reserved-memory-mb", value: "{{ yarn_nodemanager_system_reserved_memory_mb  }}" }
    - {var: "yarn.nodemanager.resource.cpu-vcores", value: "{{ yarn_nodemanager_resource_vcores  }}" }
    - {var: "yarn.nodemanager.resource.count-logical-processors-as-cores", value: "{{ yarn_nodemanager_logical_procs_as_cores }}" }
    - {var: "yarn.nodemanager.resource.pcores-vcores-multiplier", value: "{{ yarn_nodemanager_resource_pcores_multiplier }}" }
    - {var: "yarn.nodemanager.vmem-check-enabled", value: "{{ yarn_nodemanager_check_vmem }}" }
    - {var: "yarn.nodemanager.pmem-check-enabled", value: "{{ yarn_nodemanager_check_pmem }}" }
    - {var: "yarn.nodemanager.vmem-pmem-ratio", value: "{{ yarn_nodemanager_vmem_pmem_ratio  }}" }
    - {var: "yarn.nodemanager.hostname", value: "{{ inventory_hostname.split('.')[0] }}-{{ hadoop_cluster_name }}" }
    - {var: "yarn.nodemanager.bind-host", value: "{{ inventory_hostname.split('.')[0] }}-{{ hadoop_cluster_name }}" }
    notify: restart yarn
- name: configure map reduce
  hosts: nodes
  gather_facts: false
  become: true
  tasks:
  - name: get ram
    tags: configure_map_reduce
    shell: "free -m | grep Mem:|awk '{ printf $2}'"
    register: node_ram
    changed_when: false
  - name: get cpu
    tags: configure_map_reduce
    command: "nproc --all"
    register: node_cpu
    changed_when: false
  - name: copy script to update hadoop xml files
    tags: configure_map_reduce
    copy:
      src: "./xmlpresent.sh"
      dest: "/tmp/xmlpresent.sh"
      owner: hadoop
      group: hadoop
      mode: 0770
  - name: copy conf file xml files
    tags: configure_map_reduce
    copy:
      src: "{{ hadoop_conf_dir }}/mapred-site.xml.template"
      dest: "{{ hadoop_conf_dir }}/mapred-site.xml"
      remote_src: True
      force: false
      owner: yarn
      group: hadoop
      mode: 0770
  - name: update mapred-site.xml on nodes
    tags: configure_map_reduce
    command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/mapred-site.xml"
    register: out
    changed_when: not out.stdout.startswith('NO-CHANGE')
    with_items:
    - {var: "mapreduce.framework.name", value: "{{ mapreduce_framework_name }}" }
    - {var: "mapreduce.map.memory.mb", value: "{{ ((1.5*node_ram.stdout|int) / (node_cpu.stdout|int))|int }}" }
    - {var: "mapreduce.map.java.opts", value: "-Xmx{{ (node_ram.stdout|int / node_cpu.stdout|int)|int }}M" }
    - {var: "mapreduce.reduce.memory.mb", value: "{{ ((2*node_ram.stdout|int) / (node_cpu.stdout|int))|int }}" }
    - {var: "mapreduce.reduce.java.opts", value: "-Xmx{{ (2*node_ram.stdout|int / node_cpu.stdout|int)|int }}M" }
    - {var: "mapreduce.task.io.sort.mb", value: "{{ ((0.5*node_ram.stdout|int) / (node_cpu.stdout|int))|int }}" }
    - {var: "mapreduce.task.io.sort.factor", value: "{{ mapreduce_task_io_sort_factor  }}" }
    - {var: "mapreduce.reduce.shuffle.parallelcopies", value: "{{ mapreduce_reduce_shuffle_parallelcopies  }}" }
    - {var: "mapreduce.jobhistory.address", value: "{{ groups['resourcemanager'][0].split('.')[0]}}:{{mapreduce_jobhistory_port  }}" }
    - {var: "mapreduce.jobhistory.webapp.address", value: "{{ groups['resourcemanager'][0].split('.')[0]}}:{{ mapreduce_jobhistory_webapp_port }}" }
    - {var: "mapreduce.jobhistory.intermediate-done-dir", value: "{{ mapreduce_jobhistory_intermediate_done_dir  }}" }
    - {var: "mapreduce.jobhistory.done-dir", value: "{{ mapreduce_jobhistory_done_dir  }}" }
- name: hdfs is up & running 
  hosts: namenode
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: name node folder exists 
    tags: hdfs_up
    file:  
      path: "{{ hdfs_namenode_dir }}"
      state: directory
      mode: 0751
      owner: hdfs
      group: hadoop
  - name: name node formatted
    tags: hdfs_up
    command: "{{ hadoop_home }}/bin/hdfs namenode -format {{ hadoop_cluster_name }}" 
    args:
      chdir: "{{ hadoop_install }}"
      creates: "{{ hdfs_namenode_dir }}/current"
    become: true 
    become_user: hdfs
  - name: namenode service installed
    tags: hdfs_up
    template:
      src: templates/hdfs-namenode.service.j2
      dest: /lib/systemd/system/hdfs-namenode.service
      owner: root
      group: root
    register: out
  - name: reload systemctl if needed
    tags: hdfs_up
    command: systemctl daemon-reload
    when: out.changed
  - name: namenode is running
    tags: hdfs_up
    service:
      name: hdfs-namenode
      state: started
- name: data nodes atre running
  hosts: datanodes
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: data node folder exists 
    tags: hdfs_up
    file:  
      path: "{{ hdfs_datanode_dir }}"
      state: directory
      mode: 0751
      owner: hdfs
      group: hadoop
  - name: datanode service installed
    tags: hdfs_up
    template:
      src: templates/hdfs-datanode.service.j2
      dest: /lib/systemd/system/hdfs-datanode.service
      owner: root
      group: root
    register: out
  - name: reload systemctl if needed
    tags: hdfs_up
    command: systemctl daemon-reload
    when: out.changed
  - name: datanode is running
    tags: hdfs_up
    service:
      name: hdfs-datanode
      state: started
- name: yarn resource manager is up & running 
  hosts: resourcemanager
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: yarn resource manager service installed
    tags: yarn_up
    template:
      src: templates/yarn-resourcemanager.service.j2
      dest: /lib/systemd/system/yarn-resourcemanager.service
      owner: root
      group: root
    register: out
  - name: reload systemctl if needed
    tags: yarn_up
    command: systemctl daemon-reload
    when: out.changed
  - name: resource manager is running
    tags: yarn_up
    service:
      name: yarn-resourcemanager
      state: started
  - name: yarn proxy service installed
    tags: yarn_up
    template:
      src: templates/yarn-proxyserver.service.j2
      dest: /lib/systemd/system/yarn-proxyserver.service
      owner: root
      group: root
    register: out
  - name: reload systemctl if needed
    tags: yarn_up
    command: systemctl daemon-reload
    when: out.changed
  - name: proxy server is running
    tags: yarn_up
    service:
      name: yarn-proxyserver
      state: started
- name: yarn node manager is up and running
  hosts: nodemanagers
  gather_facts: false
  become: true
  roles: 
  - hadoop_commons 
  tasks:
  - name: node manager log folder exists 
    tags: yarn_up
    file:  
      path: "{{ yarn_nodemanager_local_dirs }}"
      state: directory
      mode: 0751
      owner: yarn 
      group: hadoop
  - name: yarn service installed
    tags: yarn_up
    template:
      src: templates/yarn-nodemanager.service.j2
      dest: /lib/systemd/system/yarn-nodemanager.service
      owner: root
      group: root
    register: out
  - name: reload systemctl if needed
    tags: yarn_up
    command: systemctl daemon-reload
    when: out.changed
  - name: node manager is running
    tags: yarn_up
    service:
      name: yarn-nodemanager
      state: started
- name: install spark (gateway node)
  hosts: sparkrunner
  gather_facts: false
  become: true
  roles: [sparkrunner]
  environment:
    http_proxy: "{{ proxy_host }}:{{ proxy_port }}"
    https_proxy: "{{ proxy_host }}:{{ proxy_port }}"

